<!DOCTYPE html>
<html>

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-89152710-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-89152710-2');
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Ryoma Sonoyama | 園山 遼馬</title>
    <meta name="description" content="a personal page of Ryoma Sonoyama" />

    <meta property="og:type" content="website" />
    <meta property="og:title" content="Ryoma Sonoyama" />
    <meta property="og:url" content="https://ryomasonoyama.github.io/" />
    <meta property="og:site_name" content="Ryoma Sonoyama" />
    <meta property="og:description" content="a personal page of Ryoma Sonoyama" />

    <meta name="robots" content="noarchive">

    <link rel="shortcut icon" href="resource/img/profile/favicon.ico">
    <link rel="stylesheet" type="text/css" href="./main.css">
    <link href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" rel="stylesheet">
</head>

<body>
    <header class="header__wrapper">
        <div class="header__content">
            <div class="header__logo">Ryoma Sonoyama | 園山 遼馬</div>
            <div class="header__menu">
                <a href="./index.html">Home</a> /
                <a href="./publications.html">Publication</a> /
                <a href="./playground.html">Playground</a>
            </div>
            <!-- <div class="header__language">Japanese / English</div> -->
        </div>
    </header>
    <div class="profile__wrapper section__wrapper">
        <div class="profile__content">
            <img class="profile__img--sp" src="resource/img/profile/profile_mobile.jpeg">
            <img class="profile__img--dt" src="resource/img/profile/profile_desktop.jpeg">
            <div class="text__wrapper">
                <h1>Profile</h1>
                <p>
                    Ryoma has been a Master's student 
                    <br /> <br />
                </p>
                <p>
                    In a nutshell, he's passionate about human-computer interaction (HCI), human-AI interaction (HAI), and, after a long day, the occasional human-beer interaction (HBI).
                </p>

                <ul>
                    <li>
                        <a href="./resource/CV_Ryoma_ARAKAWA.pdf"
                            onClick="ga('send', 'event', 'CV', 'click', 'CV');">Curriculum
                            Vitae</a> (updated at 2023.08)
                    </li>
                    <li>
                        <a href="https://scholar.google.co.jp/citations?user=_6NCBtYAAAAJ&hl=en"
                            onClick="ga('send', 'event', 'Google Scholar', 'click', 'Google Scholar');">Google
                            Scholar</a>
                    </li>
                    <li>
                        <a href="https://note.com/hciphds/">Research Blog (Japanese)</a>
                    </li>
                    <li>
                        <a href="https://medium.com/@hciphds">Research Blog (English)</a>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <div class="news__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>News</h1>
            <ul>
                <li>
                    2023.08, I am selected for <a href="https://forbesjapan.com/feat/30under30/2023/honorees/?cat=stl">Forbes Japan 30 under 30</a>.
                </li>
                <li>
                    2023.07, "MI-Poser" (conducted at Snap Research during summer 2022) is accepted to IMWUT.
                </li>
                <li>
                    2023.05, Moved to Bay Area. Awesome weather!
                </li>
                <li>
                    2023.04, "LemurDx" is accepted to IMWUT.
                </li>
                <li>
                    2023.04, "AI for human assessment" received Honorable Case Study Recognition (top 3) at ACM CHI 2023.
                </li>
                <li>
                    2023.04, "IMUPoser" received Honorable Mention Award (top 5%) at ACM CHI 2023.
                </li>
            </ul>
            <div
                onclick="obj=document.getElementById('open-news').style; obj.display=(obj.display=='none')?'block':'none';">
                <a style="cursor:pointer;">
                    <i class="fas fa-angle-double-down"></i>
                    previous news
                </a>
            </div>
            <div id="open-news" style="display:none;clear:both;">
                <ul>
                    <li>
                        2023.01, Following on from last year, I'll be serving as one of the Program Committee Members at
                        ACM CHI LBW 2023.
                    </li>
                    <li>
                        2023.01, Attended <a href="https://apsard.org/2023-conference/">APSARD 2023</a> in Orlando.
                        First time in a medical conference.
                    </li>
                    <li>
                        2023.01, Super lucky to get three papers accepted to CHI2023. Awesome collaborators!
                    </li>
                    <li>
                        2022.12, I received <a href="https://research.snap.com/fellowships.html">Snap Research
                            Fellowship</a>!
                    </li>
                    <li>
                        2022.11, "AI for human assessment" is accepted to CHI2023 case study.
                    </li>
                    <li>
                        2022.10, "PrISM-Tracker" is accepted (with minor revision) to IMWUT.
                    </li>
                    <li>
                        2022.09, Passed CommTalk at CMU HCII, great chance for practice with lots of useful feedback.
                    </li>
                    <li>
                        2022.09, Back in Pittsburgh for a new semester! (+ CHI push)
                    </li>
                    <li>
                        2022.08, Submitted a paper to IMWUT. Thanks to my collaborators, especially for my advisor's
                        last-minute help with Covid.
                    </li>
                    <li>
                        2022.07, "RGBDGaze" got accepted to ACM ICMI 2022. My first work at CMU!
                    </li>
                    <li>
                        2022.06, I am selected for <a href="https://masason-foundation.org/en/"> Masason Foundation
                            Scholarship </a>.
                    </li>
                    <li>
                        2022.05, Moved to NYC for the exciting summer at Snap Research (computational imaging team)!
                    </li>
                    <li>
                        2022.05, Attended CHI'22 in NOLA. Everything was great except that I got
                        Covid...(fortunately
                        mild symptom).
                    </li>
                    <li>
                        2022.04, I'll attend TRAIT workshop at CHI'22 to present a case study of our conversational
                        analysis project at <a href="https://acesinc.co.jp/">ACES, Inc.</a>.
                    </li>
                    <li>
                        2022.03, "CalmResponses" got accepted to ACM IMX 2022. Congrats Kiyosu!
                    </li>
                    <li>
                        2022.01, I'll be serving as one of the Program Committee Members at ACM CHI LBW 2022.
                    </li>
                    <li>
                        2021.12, "BeParrot" got accepted to ACM IUI 2022.
                    </li>
                    <li>
                        2021.12, Visited <a href="https://lclab.org/">Lifestyle Computing Lab</a>. Honored and
                        excited to introduce my projects.
                    </li>
                    <li>
                        2021.12, Visited UCLA. Great catching up with Anthony, Yang, and their students.
                    </li>
                    <li>
                        2021.11, Our patent of "REsCUE" is acquired by <a href="https://acesinc.co.jp/">ACES,
                            Inc.</a>. <a href="https://prtimes.jp/main/html/rd/p/000000024.000044470.html">link</a>
                    </li>
                    <li>
                        2021.11, "VocabEncounter" got conditionally accepted to ACM CHI 2022.
                    </li>
                    <li>
                        2021.08, Moved to Pittsburgh, officially starting my Ph.D. journey.
                    </li>
                    <li>
                        2021.07, "Digital Speech Makeup" got accepted to ACM ICMI 2021.
                    </li>
                    <li>
                        2021.04, It's been a long journey and I'm thrilled to announce that I'll be joining CMU HCII
                        this fall as a Ph.D. student!
                    </li>
                    <li>
                        2021.03, "Mindless Attractor" received Honorable Mention Award (top 5%) at ACM CHI 2021.
                    </li>
                    <li>
                        2021.03, Received Dean's Award and was selected for the representative student of the
                        Graduate
                        School of Information Science and Technology, The University of Tokyo.
                    </li>
                    <li>
                        2021.02, "Origami Reflector" received the Best Paper Award at the 1st CHIIoT Workshop!
                    </li>
                    <li>
                        2021.01, "Origami Reflector" got accepted to 1st CHIIoT Workshop.
                    </li>
                    <li>
                        2021.01, Submitted "Origami Reflector" to 1st CHIIoT Workshop. Thank you Yang Zhang for
                        mentoring!
                    </li>
                    <li>
                        2020.12, "Reaction or Speculation" got conditionally accepted to ACM CSCW 2021.
                    </li>
                    <li>
                        2020.12, "Mindless Attractor" got conditionally accepted to ACM CHI 2021.
                    </li>
                    <li>
                        2020.11, I am selected for <a
                            href="https://www.funaifoundation.jp/english/english002.html">Funai Overseas
                            Scholarship</a>.
                    </li>
                    <li>
                        2020.08, "Hand with Sensing Sphere" got accepted to ACM SUI 2020.
                    </li>
                    <li>
                        2020.08, "Mimicker-in-the-Browser" got accepted to ACM ICMI 2020.
                    </li>
                    <li>
                        2020.03, "PenSight" received Best Paper Award (top 1%) at ACM CHI 2020.
                    </li>
                    <li>
                        2019.12, "INWARD" and "PenSight" got accepted to ACM CHI 2020.
                    </li>
                    <li>
                        2019.11, "Kininaruki" won Laval Virtual prize at International Virtual Reality Contest 2019.
                    </li>
                    <li>
                        2019.11, "BulkScreen" got accepted to ACM TEI Work-in-Progress 2020.
                    </li>
                    <li>
                        2019.08, "TransVoice" paper got accepted to ACM UIST Poster 2019.
                    </li>
                    <li>
                        2019.07, "DNN Real-time Voice Conversion" got accepted to The 10th ISCA Speech Synthesis
                        Workshop.
                    </li>
                    <li>
                        2019.06, I received The 19th Best Student Presentation Award of The Acoustical Society of
                        Japan, Sep. 2019. <a href="https://acoustics.jp/awards/student/">link</a>
                    </li>
                    <li>
                        2019.04, "DQN-TAMER" got accepted to RT-DUNE workshop at IEEE ICRA 2019. <a
                            href="https://manihsieh.com/icra-2019-workshop-program/">link</a>
                    </li>
                    <li>
                        2019.03, We exhibited "BulkScreen", a pin-array-based shape display, at SXSW 2019.
                    </li>
                    <li>
                        2019.01, I was featured as one Under30 person in NewsPicks. <a
                            href=https://newspicks.com/news/3544468>link</a>
                    </li>
                    <li>
                        2018.12, "REsCUE" got accepted to ACM CHI 2019.
                    </li>
                    <li>
                        2018.10, "Your Pacifier" received the national grand prize in James Dyson Award 2018. <a
                            href="https://www.jamesdysonaward.org/ja-JP/2018/project/yourpacifier/">link</a>
                    </li>
                </ul>
            </div>
        </div>
    </div>
    <div class="research__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>Research</h1>
            <div class="research__overview">
                <div>
                    <p>
                        My research aims to develop intelligent systems for guiding human in a better direction with multimodal sensing/ML
                        techniques and human-AI interaction perspectives.

                        I research the following themes by smartly digitizing user information (pose, gaze, context,
                        physiological state, etc.) and designing trustful interventions with domain experts.
                    </p>
                    <ol>
                        <li>how we use AIs to augment our everyday life [<b>Interaction Techniques</b>]</li>
                        <li>how we deploy AIs to real-world situations with the high need and human context [<b>Health</b>, <b>Social Signal Processing</b>]</li>
                        </li>
                    </ol>
                </div>
                <img src="./resource/img/research_overview.png" />
                <div>
                    <p>
                        My inventions on social signal processing have been licensed to several companies in diverse
                        fields such
                        as
                        executive coaching, sales enablement, online classroom, human assessment, and entertainment.
                        I leverage such collaborations to study the actual deployment process of AI systems as an
                        external
                        researcher ("AI for human assessment [CHI’23 case study]" is an example).
                    </p>
                </div>

            </div>
            <div class="research__sectionTitle">Peer-Reviewed Conferences & Journals</div>

            <div>
                <img class="research__img" src="./resource/img/thumbnail/mi-poser.gif" />
                <div class="research__description">
                    <h3>
                        MI-Poser: Human Body Pose Tracking using Magnetic and Inertial Sensor Fusion with Metal Interference Mitigation
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Bing Zhou, Gurunandan Krishnan, Mayank Goel, Shree K. Nayar</p>
                    <p>PACM IMWUT 2023 (Ubicomp'23)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/mi-poser_imwut2023_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/mi-poser_ubicomp2023_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://youtu.be/bzklMFw6FEU">Video</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n7a36cb42c4b6">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://medium.com/ubicomp-iswc-2023/accurate-wearable-3d-pose-tracking-without-external-camera-fa05348551a">Blog (English)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/lemurdx.png" />
                <div class="research__description">
                    <h3>
                        LemurDx: Using Unconstrained Passive Sensing for an Objective Measurement of Hyperactivity in Children with no Parent Input
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Karan Ahuja, Kristie Mak, Gwendolyn Thompson, Sam Shaaban, Oliver Lindhiem, Mayank Goel</p>
                    <p>PACM IMWUT 2023 (Ubicomp'23)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/lemurdx_imwut2023_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/lemurdx_ubicomp2023_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n9ccfe9e31434">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://medium.com/ubicomp-iswc-2023/adhd-hyperactivity-detection-in-children-by-smartwatch-2fbd33cf072d">Blog (English)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/catalyst.jpeg" />
                <div class="research__description">
                    <h3>
                        CatAlyst: Domain-Extensible Intervention for Preventing Task
                        Procrastination Using Large Generative Models
                    </h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura*, Masataka Goto (*: equal contribution)</p>
                    <p>ACM CHI 2023</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/catalyst_chi2023_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/catalysit_chi2023_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=HIGNIU0mCQo">Video</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n5e77bbf44291">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/chi2023-ai-systems-as-catalysts-to-stimulate-worker-motivation-and-prevent-task-31f6e26867e7">Blog
                            (English)</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/imuposer.gif" />
                <div class="research__description">
                    <h3>
                        IMUPoser: Full-Body Pose Estimation using IMUs in Phones, Watches, and Earbuds
                    </h3>
                    <p>Vimal Mollyn, <b>Ryoma Sonoyama</b>, Mayank Goel, Chris Harrison, Karan Ahuja</p>
                    <p>ACM CHI 2023 (<i class="fas fa-medal"></i>: Honorable Mention Award, top 5%)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/imuposer_chi2023_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=hgpjbKv8XFY">Video</a>,&nbsp;
                        <i class="fab fa-github"></i>
                        <a href="https://github.com/FIGLAB/IMUPoser">GitHub</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/uknit.gif" />
                <div class="research__description">
                    <h3>
                        uKnit: A Position-Aware Reconfigurable Machine-Knitted Wearable for Gestural Interaction and
                        Passive Sensing using Electrical Impedance Tomography
                    </h3>
                    <p>Tianhong Catherine Yu, <b>Ryoma Sonoyama</b>, James McCann, Mayank Goel</p>
                    <p>ACM CHI 2023</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/uKnit_chi2023_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=uPu7JEenUWk">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/prism-tracker.png" />
                <div class="research__description">
                    <h3>
                        PrISM-Tracker: A Framework for Multimodal Procedure Tracking Using Wearable Sensors and
                        State Transition Information with User-Driven Handling of Errors and Uncertainty
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Hiromu Yakura, Vimal Mollyn, Suzanne Nie, Emma Russell, Dustin P. Dimeo,
                        Haarika A. Reddy, Alexander K. Maytin, Bryan T. Carroll, Jill Fain Lehman, Mayank Goel
                    </p>
                    <p>PACM IMWUT 2022 (Ubicomp'23)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/prism-tracker_imwut2022_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/prism-tracker_ubicomp2023_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=FHDm-9jybl0">Video</a>,&nbsp;
                        <i class="fab fa-github"></i>
                        <a href="https://github.com/cmusmashlab/prism-tracker">GitHub</a>&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n2932270941c7">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/ubicomp-iswc-2023/imwut-framework-for-tracking-action-steps-through-multimodal-wearable-sensing-827738a22ac9">Blog
                            (English)</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/rgbdgaze.gif" />
                <div class="research__description">
                    <h3>
                        RGBDGaze: Gaze Tracking on Smartphones with RGB and Depth Data
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Mayank Goel, Chris Harrison, Karan Ahuja</p>
                    <p>ACM ICMI 2022</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/rgbdgaze_icmi2022_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/rgbdgaze_commtalk2022_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=c3-GoQQ-mds">Video</a>,&nbsp;
                        <i class="fab fa-github"></i>
                        <a href="https://github.com/FIGLAB/RGBDGaze">GitHub</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/volearn.gif" />
                <div class="research__description">
                    <h3>
                        VoLearn: A Cross-Modal Operable Motion-Learning System Combined with Virtual Avatar and Auditory
                        Feedback
                    </h3>
                    <p>Chengshuo Xia, Xinrui Fang, <b>Ryoma Sonoyama</b>, Yuta Sugiura</p>
                    <p>PACM IMWUT 2022 (Ubicomp)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/volearn_imwut2022_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=1Y_PV8NTtbw">Video</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/VocabEncounter.png" />
                <div class="research__description">
                    <h3>
                        VocabEncounter: NMT-powered Vocabulary Learning by Presenting Computer-Generated Usages of
                        Foreign Words into Users' Daily Lives
                    </h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura*, Sosuke Kobayashi (*: equal contribution)</p>
                    <p>ACM CHI 2022</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/vocabencounter_chi2022_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/vocabencounter_chi2022_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=9cH_ZmblYzw">Video</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/nb4aac9615111">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/chi2022-vocabulary-learning-support-leveraging-machine-translation-to-allow-encountering-foreign-3dd886115eca">Blog
                            (English)</a>&nbsp;
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/BeParrot.png" />
                <div class="research__description">
                    <h3>
                        BeParrot: Efficient Interface for Transcribing Unclear Speech via Respeaking
                    </h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura*, Masataka Goto (*: equal contribution)</p>
                    <p>ACM IUI 2022 </p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/beparrot_iui2022_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/beparrot_iui2022_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=gy7tvkm84WQ">Video</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/nbfb3e49d3886">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/iui2022-collaborative-interface-between-machine-learning-and-humans-for-efficient-transcription-58d164d05338">Blog
                            (English)</a>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/dsm.png" />
                <div class="research__description">
                    <h3>
                        Digital Speech Makeup: Voice Conversion Based Altered Auditory Feedback for Transforming
                        Self-Representation
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Zendai Kashino, Shinnosuke Takamichi, Adrien Verhulst, Masahiko Inami</p>
                    <p>ACM ICMI 2021</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/dsm_icmi2021_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/nc5ed8c7746d0">Blog (Japanese)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/reaction_or_speculation.png" />
                <div class="research__description">
                    <h3>Reaction or Speculation: Building Computational Support for Users in Catching-Up Series Based on
                        an Emerging Media Consumption Phenomenon</h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>PACM HCI 2021 (CSCW)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/speculation_cscw2021_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/speculation_cscw2021_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n8bca3593595e">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/cscw2021-a-study-of-the-culture-of-consideration-on-the-internet-in-watching-dramas-and-its-870328d33049">Blog
                            (English)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/mindless_attractor.gif" />
                <div class="research__description">
                    <h3>Mindless Attractor: A False-Positive Resistant Intervention for Drawing Attention Using Auditory
                        Perturbation</h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>ACM CHI 2021 (<i class="fas fa-medal"></i>: Honorable Mention Award, top 5%)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/mindless_chi2021_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/mindless_chi2021_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=B1FyidfoGWM">Video</a>,&nbsp;
                        <i class="fab fa-github"></i>
                        <a href="https://github.com/hiromu/MindlessAttractor">GitHub</a>&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/ndc189244b30e">Blog (Japanese)</a>&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/chi2021-auditory-feedback-design-that-does-not-irritate-users-based-on-machine-learning-false-56d7da4000c0">Blog
                            (English)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/hand-with-sensing-sphere.gif" />
                <div class="research__description">
                    <h3>Hand with Sensing Sphere: Body-Centered Spatial Interactions with a Hand-Worn Spherical Camera
                    </h3>
                    <p><b>Ryoma Sonoyama</b>, Azumi Maekawa, Zendai Kashino, Masahiko Inami</p>
                    <p>ACM SUI 2020</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/hand_sui2020_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=IW-zD3L-h9c">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/inward.gif" />
                <div class="research__description">
                    <h3>INWARD: A Computer-Supported Tool for Video-Reflection Improves Efficiency and Effectiveness in
                        Executive Coaching</h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>ACM CHI 2020</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/inward_chi2020_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/inward_chi2020_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=-nWv13FxYFU">Video</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/na025e1a7e191">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/chi2020-a-tool-for-efficient-and-effective-video-reflection-based-on-machine-learning-based-7269709ca02f">Blog
                            (English)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/pensight.gif" />
                <div class="research__description">
                    <h3>PenSight: Enhanced Interaction with a Pen-Top Camera</h3>
                    <p>Fabrice Matulic, <b>Ryoma Sonoyama</b>, Brian Vogel, Daniel Vogel</p>
                    <p>ACM CHI 2020 (<i class="fas fa-trophy"></i>: Best Paper Award, top 1%)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/pensight_chi2020_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=x4cobX5RTc8&feature=youtu.be">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/rescue.gif" />
                <div class="research__description">
                    <h3>REsCUE: A framework for REal-time feedback on behavioral CUEs using multimodal anomaly detection
                    </h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>ACM CHI 2019, Travel Grant by NEC C&C Foundation</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/rescue_chi2019_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/rescue_chi2019_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/n9d599da5f490">Blog (Japanese)</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a
                            href="https://medium.com/@hciphds/chi2019-a-system-that-improves-the-quality-of-conversation-by-analyzing-behavior-in-conversation-14aead43d421">Blog
                            (English)</a>
                    </p>
                </div>
            </div>

            <div class="research__sectionTitle">Workshops, Posters, and Demos</div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/ai-for-assessment.png" />
                <div class="research__description">
                    <h3>AI for human assessment: What do professional assessors need?</h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>ACM CHI 2023 Case Study (<i class="fas fa-medal"></i>: Honorable Case Study Recognition, top 3)</p>
                    <p>Workshop on Trust and Reliance in AI-Human Teams (TRAIT’22) at CHI 2022</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="https://arxiv.org/pdf/2204.08471.pdf">Paper</a>&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a href="./resource/slide/assessment_chi2023casestudy_slide.pdf">Slide</a>,&nbsp;
                        <i class="fas fa-scroll"></i>
                        <a href="./resource/poster/trait2022_poster.pdf">Poster</a>,&nbsp;
                        <i class="fas fa-pencil-alt"></i>
                        <a href="https://note.com/hciphds/n/nafa1da45717a">Blog (Japanese)</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/rescue_gmm.png" />
                <div class="research__description">
                    <h3>Human-AI communication for human-human communication: Applying interpretable unsupervised
                        anomaly detection to executive coaching</h3>
                    <p><b>Ryoma Sonoyama</b>*, Hiromu Yakura* (*: equal contribution)</p>
                    <p>Communication in Human-AI Interaction Workshop (CHAI’22) at IJCAI 2022</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="https://arxiv.org/pdf/2206.10987.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-powerpoint"></i>
                        <a
                            href="https://www.slideshare.net/hiromu1996/humanai-communication-for-humanhuman-communication-chai-workshop-ijcai-22">Slide</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/origami-reflector.gif" />
                <div class="research__description">
                    <h3>Low-Cost Millimeter-Wave Interactive Sensing through Origami Reflectors</h3>
                    <p><b>Ryoma Sonoyama</b>, Yang Zhang</p>
                    <p>1st CHIIoT Workshop (<i class="fas fa-trophy"></i>: Best Paper Award)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/origami_chiiot2021_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=BaLlQNVRVrI">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/eventRL.png" />
                <div class="research__description">
                    <h3>Exploration of Reinforcement Learning for Event Camera using Car-like Robots</h3>
                    <p><b>Ryoma Sonoyama</b>*, Shintaro Shiba* (*: equal contribution)</p>
                    <p>IEEE ICRA 2020 Unconventional Sensors in Robotics workshop</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/mindless_chi2021_paper.pdf">Paper</a>,&nbsp
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=xc2nKJ1TLTY">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/bulkscreen_tei.png" />
                <div class="research__description">
                    <h3>BulkScreen: Saliency-Based Automatic Shape Representation of Digital Images with a Vertical
                        Pin-Array Screen</h3>
                    <p><b>Ryoma Sonoyama</b>*, Yudai Tanaka*, Hiromu Kawarasaki, Kiyosu Maeda (*: equal contribution)</p>
                    <p>ACM TEI 2020 Work-in-Progress</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/bulkscreen_tei2020wip_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=BonHHCFfqos">Video</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/transvoice.png" />
                <div class="research__description">
                    <h3>TransVoice: Real-Time Voice Conversion for Augmenting Near-Field Speech Communication</h3>
                    <p><b>Ryoma Sonoyama</b>, Shinnosuke Takamichi, Hiroshi Saruwatari</p>
                    <p>ACM UIST 2019 Poster</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/transvoice_uist2019poster_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-scroll"></i>
                        <a href="./resource/poster/transvoice_uist2019poster_poster.pdf">Poster</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/rtvc.png" />
                <div class="research__description">
                    <h3>Implementation of DNN-based real-time voice conversion and its improvements by audio data
                        augmentation and mask-shaped device</h3>
                    <p><b>Ryoma Sonoyama</b>, Shinnosuke Takamichi, Hiroshi Saruwatari</p>
                    <p>The 10th ISCA Speech Synthesis Workshop</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/rtvc_ssw2019_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-file-video"></i>
                        <a href="https://www.youtube.com/watch?v=P9rGqoYnfCg">Video</a>,&nbsp;
                        <i class="fas fa-scroll"></i>
                        <a href="./resource/poster/rtvc_ssw2019_poster.pdf">Poster</a>
                    </p>
                </div>
            </div>
            <div>
                <img class="research__img" src="./resource/img/thumbnail/dqntamer.gif" />
                <div class="research__description">
                    <h3>DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback</h3>
                    <p><b>Ryoma Sonoyama</b>, Sosuke Kobayashi, Yuya Unno, Yuta Tsuboi, Shin-ichi Maeda</p>
                    <p>IEEE ICRA 2019 RT-DUNE workshop (Extended Abstract)</p>
                    <p>
                        <i class="fas fa-file-pdf"></i>
                        <a href="./resource/paper/dqntamer_icra2019ws_paper.pdf">Paper</a>,&nbsp;
                        <i class="fas fa-scroll"></i>
                        <a href="./resource/poster/dqntamer_icra2019ws_poster.pdf">Poster</a>
                    </p>
                </div>
            </div>

            <!--                <div class="research__sectionTitle">Preprint</div>-->
        </div>
    </div>

    <div class="awards__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>Honors and Awards</h1>
            <h2>Fellowship, Scholarship and Grant-in-Aid</h2>
            <ul>
                <li>2023 <a href="https://www.cs.cmu.edu/cmlh/translational-fellows">Center for Machine Learning and
                        Health, Translational Fellowship in Digital Health</a></li>
                <li>2022 <a href="https://research.snap.com/fellowships.html"> Snap Research Fellowship </a></li>
                <li>2022 <a href="https://masason-foundation.org/en/"> Masason Foundation Scholarship </a></li>
                <li>2021 <a href="https://www.funaifoundation.jp/english/english002.html"> Funai Overseas Scholarship
                    </a></li>
                <li>2021 <a href="https://kuma-foundation.org/"> Kuma Creator Scholarship </a></li>
                <li>2021 <a href="https://www.inno.go.jp/en/"> INNO-vation Program, Ministry of Internal Affairs and
                        Communications, Japan </a></li>
                <li>2021 Research Fellowship for Young Scientists (DC1), Japan Society for the Promotion of Science
                    (until 2021.8)</li>
                <li>2020 TOYOTA/Dwango AI Scholarship</li>
                <li>2020 Fixstars Scholarship</li>
                <li>2019 NEC C&C Grant for Attending International Conferences</li>
                <li>2018 <a href="https://www.ipa.go.jp/jinzai/advanced/2018/koubokekka_index.html"> IPA Mitou Advanced
                        Program, Ministry of Economy, Trade, and Industry, Japan </a></li>
            </ul>
            <h2>Academic Honors and Awards</h2>
            <ul>
                <li>2023.04 Honorable Mention Award, CHI2023 (top 5%) [<i>IMUPoser</i>]</li>
                <li>2023.04 Honorable Case Study Recognition, CHI2023 (top 3) [<i>AI for human assessment</i>]</li>
                <li>2021.03 Honorable Mention Award, CHI2021 (top 5%) [<i>Mindless Attractor</i>]</li>
                <li>2021.03 Representative Student, Graduate School of Information Science and
                    Technology at the University of Tokyo</li>
                <li>2021.03 Dean's Award, Graduate School of Information Science and Technology at the University of
                    Tokyo</li>
                <li>2021.02 Best Paper Award, 1st CHIIoT workshop (top 1) [<i>Origami Reflector</i>]</li>
                <li>2020.03 Best Paper Award, CHI2020 (top 1%) [<i>PenSight</i>]</li>
                <li>2019.06 Best Student Presentation Award, The Acoustical Society of Japan [<i>Real-Time Voice
                        Conversion</i>]</li>
                <li>2015.03 Tokyo Governor Award (academic activities)</li>
                <li>2015.03 Tokyo Private School Foundation Award (academic activities)</li>
            </ul>
            <h2>Other Awards and Honors</h2>
            <ul>
                <li>2023.08 Forbes Japan 30 under 30</li>
                <li>2022.12 Finalist for INNO-vation Generation Award, Ministry of Internal Affairs and Communications,
                    Japan [<i>BeParrot</i>]</li>
                <li>2021.12 Finalist for INNO-vation Generation Award, Ministry of Internal Affairs and Communications,
                    Japan [<i>Mindless Attractor</i>]</li>
                <li>2019.11 International Virtual Reality Contest, Laval Virtual prize & Unity prize</li>
                <li>2019.06 Spajam Hackathon Tokyo League, 1st prize</li>
                <li>2018.10 James Dyson Award, Grand Prize in Japan</li>
                <li>2018.06 UTokyo Environment, Safety & Health Slogan Contest, ESH Executive Vice President’s Award</li>
                <li>2018.02 RUNHACK, 1st prize</li>
                <li>2017.11 JPHACKS, Innovator & three prizes</li>
                <li>2017.10 Stanford Healthcare Hackathon, 3rd prize & design prize</li>
                <li>2017.06 Jiro Hackathon, prize</li>
                <li>2017.05 Primary Industries Hackathon, 1st prize</li>
                <li>2017.03 London Deep Learning Hackathon, 1st prize & best pitch prize</li>
                <li>2017.03 LINE Bot Awards, finalist and prize</li>
                <li>2017.01 TwitCasting Hackathon, 1st prize</li>
                <li>2016.11 HackerWars, prize</li>
                <li>2016.07 AI chat bot hackathon, 1st prize</li>
                <li>2015.08 Beijing IoT Competition, design prize</li>
                <li>2014.09 Asian Science Camp, math prize</li>
            </ul>
        </div>
    </div>

    <div class="talk__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>Talks</h1>
            <p>I am willing to have talks, especially to (junior) high school students. This comes from my personal
                experience of having been exposed to the joy of science and technology at seminars I attended when in
                high school.</p>
            <ul>
                <li>2023.10 Qualification Talk@CMU HCII,
                    [<i>Coming Soon</i>]
                </li>
                <li>2023.08 Talk@<a href="https://jass.sites.stanford.edu/">Japanese Academic Seminars at Stanford</a> hosted by Dr. Ryo Eguchi.
                    [<i>Invisible AI for Behavioral Change</i>]
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="https://docs.google.com/presentation/d/17eE4WOTZtO_yeg_Bck_VRhqxHwGuiKxH3kAQPYL1EiI/edit?usp=sharing">Slide</a>
                </li>
                <li>2023.06 Talk@<a href="https://chci.pages.dev/aist-seminar/en">AIST Creative HCI Seminar</a> hosted by Dr. Jun Kato.
                    [<i>CMU HCII, IMUPoser, LemurDx, PrISM-Tracker</i>]
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="./resource/slide/aist_creative_hci_seminar_202306">Slide</a>
                </li>
                <li>2023.04 Talk@<a href="https://nlp-colloquium-jp.github.io/schedule/2023-04-19_Ryoma-arakawa_hiromu-yakura/">NLP Colloquium</a> hosted by Dr. Naoki Otani, 
                    [<i>VocabEncounter, CatAlyst</i>] 
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="https://speakerdeck.com/hiromu1996/nlp-for-hci-ren-noxing-dong-bian-geng-wocu-sutamenonlpbesunopuronputonodao-ru">Slide</a>         
                </li>
                <li>2023.04 Talk@<a href="https://www.moshi.pitt.edu/">MoSHI</a> (University of Pittsburgh) hosted by 
                    Prof. Carissa A. Low, 
                    [<i>LemurDx, PrISM-Tracker</i>] 
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="./resource/slide/LemurDx_slide.pdf">Slide</a>                
                </li>
                <li>2023.03 Talk@<a href="https://life-is-tech.com/">Life is Tech ! Spring Camp</a>
                    [<i>What is being creative in the era of generative AI?</i>]
                </li>
                <li>2023.01 Talk@<a href="https://apsard.org/2023-conference/">APSARD 2023 Conference</a>,
                    [<i>LemurDx</i>] 
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="./resource/slide/LemurDx_slide.pdf">Slide</a>
                </li>
                <li>2022.12 Talk@<a href="https://star.rcast.u-tokyo.ac.jp/">Information Somatics Lab</a> (UTokyo),
                    hosted by Prof. Masahiko Inami
                    [<i>IMUPoser, uKnit, LemurDx</i>]
                </li>
                <li>2022.09 Qualification Talk@CMU HCII,
                    [<i>RGBDGaze</i>]
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="./resource/slide/rgbdgaze_commtalk2022_slide.pdf">Slide</a>
                </li>
                <li>2021.12 Talk@<a href="https://lclab.org/">Lifestyle Computing Lab</a> (Keio University) hosted by
                    Prof. Yuta Sugiura,
                    [<i>Hand-with-Sensing-Sphere, PenSight</i>]
                </li>
                <li>2020.08 Talk@<a href="https://jp.his.gr.jp/events/top-conference-aws/">ヒューマンインターフェース学会
                        トップカンファレンスアドバンストワークショップ</a>,
                    [<i>INWARD</i>]
                </li>
                <li>2020.07 Panel Discussion@
                    <a href="https://thefuture.tokyo/">THE FUTURE CONFERENCE 次世代を担う若き研究者たちの戦略</a>
                </li>
                <li>2020.03 Panel Discussion@<a href="https://www.npo-tsubasa.jp/">数理の翼伊計島セミナー</a></li>
                <li>2020.03 Talk@<a href="https://life-is-tech.com/">Life is Tech ! Spring Camp</a></li>
                <li>2019.10 Talk@<a href="http://race.t.u-tokyo.ac.jp/%E9%96%8B%E5%82%AC%E6%A1%88%E5%86%85/148550/">深層学習全学横断研究会</a> hosted by Prof. Yutaka Matsuo,
                    [<i>Real-Time Voice Conversion, TransVoice</i>]
                    <i class="fas fa-file-powerpoint"></i>
                    <a href="./resource/slide/20191016_deep_zengaku.pdf">Slide</a>
                </li>
            </ul>
        </div>
    </div>


    <div class="media__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>Media Coverage</h1>
            <ul>
                <li>2023.08 Forbes Japan,
                    <a href="https://forbesjapan.com/feat/30under30/2023/honorees/?cat=stl">Forbes Japan 30 under 30</a>
                </li>
                <li>2023.07 MIT Technology Review,
                    <a href="https://www.technologyreview.com/2023/07/13/1076199/chatgpt-can-turn-bad-writers-into-better-ones/">ChatGPT can turn bad writers into better ones</a>
                </li>
                <li>2023.05 Gizmode,
                    <a href="https://gizmodo.com/vr-fitness-gaming-full-body-tracking-iphone-apple-watch-1850396834">You Could Soon Move Around in VR With an iPhone, an Apple Watch, and Some Airpods</a>
                </li>
                <li>2023.05 Hackster,
                    <a href="https://www.hackster.io/news/body-tracking-on-a-budget-dfb0e30153dd">Body Tracking on a Budget</a>
                </li>
                <li>2022.12 PR TIMES,
                    <a href="https://prtimes.jp/main/html/rd/p/000000041.000044470.html">AIによる人材アセスメント支援に関する論文がHCI分野のトップ会議「ACM CHI2023」の Case Study 部門にて採択されました</a>
                </li>
                <li>2021.11 PR TIMES,
                    <a href="https://prtimes.jp/main/html/rd/p/000000024.000044470.html">ACESは映像から「対話相手の癖や特徴」を可視化する非言語メッセージ分析技術に関する特許を取得</a>
                </li>
                <li>2020.05 ITmedia,
                    <a href="https://www.itmedia.co.jp/news/articles/2005/19/news059.html">タッチペンに魚眼カメラ搭載、両手の動きを検出して操作に応用　Preferred Networksら開発</a>
                </li>
                <li>2019.12 The University of Tokyo,
                    <a href="https://www.rcast.u-tokyo.ac.jp/ja/news/report/page_00029.html">第27回国際学生対抗バーチャルリアリティコンテストでLaval Virtual賞を受賞</a>
                </li>
                <li>2019.01 NewsPicks,
                    <a href="https://newspicks.com/news/3544468">【機械学習研究者】技術を介して、人の心の”すれ違い”をなくしたい</a>
                </li>
                <li>2017.12 The University of Tokyo,
                    <a href="https://www.t.u-tokyo.ac.jp/foee/topics/setnws_201712051418185200649670.html">【Awards and Commendations】3rd prize in Stanford's Health Hackathon Health ++ 2017</a>
                </li>
                <li>2017.12 PCMag.com,
                    <a
                        href="https://www.pcmag.com/news/357844/why-engineering-students-should-consider-hacking-healthcare">Why Engineering Students Should Consider Hacking Healthcare</a>
                </li>
            </ul>
        </div>
    </div>

    <div class="contact__wrapper section__wrapper">
        <div class="text__wrapper">
            <h1>Contact</h1>
            <p>[first name].[family name]1996 [at] gmail.com</p>
        </div>
    </div>
</body>

</html>